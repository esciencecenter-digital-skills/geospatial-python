{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66aab66c",
   "metadata": {},
   "source": [
    "-   How can I parallelize computations on rasters with Dask?\n",
    "-   How can I determine if parallelization improves calculation speed?\n",
    "-   What are good practices in applying parallelization to my raster calculations?\n",
    "\n",
    "-   Profile the timing of the raster calculations.\n",
    "-   Open raster data as a chunked array.\n",
    "-   Recognize good practices in selecting proper chunk sizes.\n",
    "-   Setup raster calculations that take advantage of parallelization.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Very often raster computations involve applying the same operation to different pieces of data. Think, for instance, to\n",
    "the “pixel”-wise sum of two raster datasets, where the same sum operation is applied to all the matching grid-cells of\n",
    "the two rasters. This class of tasks can benefit from chunking the input raster(s) into smaller pieces: operations on\n",
    "different pieces can be run in parallel using multiple computing units (e.g., multi-core CPUs), thus potentially\n",
    "speeding up calculations. In addition, working on chunked data can lead to smaller memory footprints, since one\n",
    "may bypass the need to store the full dataset in memory by processing it chunk by chunk.\n",
    "\n",
    "In this episode, we will introduce the use of Dask in the context of raster calculations. Dask is a Python library for\n",
    "parallel and distributed computing. It provides a framework to work with different data structures, including chunked\n",
    "arrays (Dask Arrays). Dask is well integrated with (`rio`)`xarray`, which can use Dask arrays as underlying\n",
    "data structures.\n",
    "\n",
    "## Dask\n",
    "\n",
    "This episode shows how Dask can be used to parallelize operations on local CPUs. However, the same library can be\n",
    "configured to run tasks on large compute clusters.\n",
    "\n",
    "More resources on Dask:\n",
    "- [Dask](https://dask.org) and [Dask Array](https://docs.dask.org/en/stable/array.html).\n",
    "- [Xarray with Dask](https://xarray.pydata.org/en/stable/user-guide/dask.html).\n",
    "\n",
    "It is important to realize, however, that many details determine the extent to which using Dask’s chunked arrays instead\n",
    "of regular Numpy arrays leads to faster calculations (and lower memory requirements). The actual operations to carry\n",
    "out, the size of the dataset, and parameters such as the chunks’ shape and size, all affects the performance of our\n",
    "computations. Depending on the specifics of the calculations, serial calculations might actually turn out to be faster!\n",
    "Being able to profile the computational time is thus essential, and we will see how to do that in a Jupyter environment\n",
    "in the next section.\n",
    "\n",
    "The example that we consider here is the application of a median filter to a satellite image.\n",
    "[Median filtering](https://en.wikipedia.org/wiki/Median_filter) is a common noise removal technique which\n",
    "replaces a pixel’s value with the median value computed from its surrounding pixels.\n",
    "\n",
    "## Time profiling in Jupyter\n",
    "\n",
    "## Introduce the Data\n",
    "\n",
    "We’ll continue from the results of the satellite image search that we have carried out in an exercise from\n",
    "[a previous episode](./05-access-data.md). We will load data starting from the `search.json` file.\n",
    "\n",
    "If you would like to work with the data for this lesson without downloading data on-the-fly, you can download the\n",
    "raster data using this [link](https://figshare.com/ndownloader/files/36028100). Save the `geospatial-python-raster-dataset.tar.gz`\n",
    "file in your current working directory, and extract the archive file by double-clicking on it or by running the\n",
    "following command in your terminal `tar -zxvf geospatial-python-raster-dataset.tar.gz`. Use the file `geospatial-python-raster-dataset/search.json`\n",
    "(instead of `search.json`) to get started with this lesson.\n",
    "\n",
    "Let’s set up a raster calculation using assets from our previous search of satellite scenes. We first load the item\n",
    "collection using the `pystac` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c65f05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystac\n",
    "items = pystac.ItemCollection.from_file(\"search.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43768a5",
   "metadata": {},
   "source": [
    "We select the last scene, and extract the URL of the true-color image (“visual”):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb060665",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets = items[-1].assets  # last item's assets\n",
    "visual_href = assets[\"visual\"].href  # true color image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc137d7",
   "metadata": {},
   "source": [
    "The true-color image is available as a raster file with 10 m resolution and 3 bands (you can verify this by opening the\n",
    "file with `rioxarray`), which makes it a relatively large file (few hundreds MBs). In order to keep calculations\n",
    "“manageable” (reasonable execution time and memory usage) we select here a lower resolution version of the image, taking\n",
    "advantage of the so-called “pyramidal” structure of cloud-optimized GeoTIFFs (COGs). COGs, in fact, typically include\n",
    "multiple lower-resolution versions of the original image, called “overviews”, in the same file. This allows us to avoid\n",
    "downloading high-resolution images when only quick previews are required.\n",
    "\n",
    "Overviews are often computed using powers of 2 as down-sampling (or zoom) factors. So, typically, the first level\n",
    "overview (index 0) corresponds to a zoom factor of 2, the second level overview (index 1) corresponds to a zoom factor\n",
    "of 4, and so on. Here, we open the third level overview (zoom factor 8) and check that the resolution is about 80 m:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26859806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "visual = rioxarray.open_rasterio(visual_href, overview_level=2)\n",
    "print(visual.rio.resolution())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5086e563",
   "metadata": {},
   "source": [
    "``` output\n",
    "(79.97086671522214, -79.97086671522214)\n",
    "```\n",
    "\n",
    "Let’s make sure the data has been loaded into memory before proceeding to time profile our raster calculation. Calling\n",
    "the `.load()` method of a DataArray object triggers data loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42383042",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual = visual.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f80e4ce",
   "metadata": {},
   "source": [
    "Note that by default data is loaded using Numpy arrays as underlying data structure. We can visualize the raster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621d7522",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual.plot.imshow(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bb73f7",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"attachment:fig/E11/true-color-image.png\" alt=\"true color image scene\" />\n",
    "<figcaption>Scene’s true-color image</figcaption>\n",
    "</figure>\n",
    "\n",
    "Let’s now apply a median filter to the image while keeping track of the execution time of this task. The filter is\n",
    "carried out in two steps: first, we define the size and centering of the region around each pixel that will be\n",
    "considered for the median calculation (the so-called “windows”), using the `.rolling()` method. We choose here windows\n",
    "that are 7 pixel wide in both x and y dimensions, and, for each window, consider the central pixel as the window target.\n",
    "We then call the `.median()` method, which initiates the construction of the windows and the actual calculation.\n",
    "\n",
    "For the time profiling, we make use of the Jupyter magic `%%time`, which returns the time required to run the content\n",
    "of a cell (note that commands starting with `%%` needs to be on the first line of the cell!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "median = visual.rolling(x=7, y=7, center=True).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adbf171",
   "metadata": {},
   "source": [
    "``` output\n",
    "CPU times: user 15.6 s, sys: 3.2 s, total: 18.8 s\n",
    "Wall time: 19.6 s\n",
    "```\n",
    "\n",
    "Let’s note down the calculation’s “Wall time” (actual time to perform the task). We can inspect the image resulting\n",
    "after the application of median filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd532cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "median.plot.imshow(robust=True, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caf9965",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"attachment:fig/E11/true-color-image_median-filter.png\" alt=\"median filter true color image\" />\n",
    "<figcaption>True-color image after median filtering</figcaption>\n",
    "</figure>\n",
    "\n",
    "## Handling edges\n",
    "\n",
    "By looking closely, you might notice a tiny white edge at the plot boundaries. These are the pixels that are less than\n",
    "3 pixels away from the border of the image. These pixels cannot be surrounded by a 7 pixel wide window. The default\n",
    "behaviour is to assign these with nodata values.\n",
    "\n",
    "Finally, let’s write the data to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76e975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "median.rio.to_raster(\"visual_median-filter.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce6a6e8",
   "metadata": {},
   "source": [
    "In the following section we will see how to parallelize these raster calculations, and we will compare timings to the\n",
    "serial calculations that we have just run.\n",
    "\n",
    "## Dask-powered rasters\n",
    "\n",
    "### Chunked arrays\n",
    "\n",
    "As we have mentioned, `rioxarray` supports the use of Dask’s chunked arrays as underlying data structure. When opening\n",
    "a raster file with `open_rasterio` and providing the `chunks` argument, Dask arrays are employed instead of regular\n",
    "Numpy arrays. `chunks` describes the shape of the blocks which the data will be split in. As an example, we\n",
    "open the blue band raster using a chunk shape of `(1, 4000, 4000)` (block size of `1` in the first dimension and\n",
    "of `4000` in the second and third dimensions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b2a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_band_href = assets[\"blue\"].href\n",
    "blue_band = rioxarray.open_rasterio(blue_band_href, chunks=(1, 4000, 4000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42a827c",
   "metadata": {},
   "source": [
    "Xarray and Dask also provide a graphical representation of the raster data array and of its blocked structure.\n",
    "\n",
    "<figure>\n",
    "<img src=\"attachment:fig/E11/xarray-with-dask.png\" alt=\"DataArray with Dask\" />\n",
    "<figcaption>Xarray Dask-backed DataArray</figcaption>\n",
    "</figure>\n",
    "\n",
    "### Exercise: Chunk sizes matter\n",
    "\n",
    "We have already seen how COGs are regular GeoTIFF files with a special internal structure. other feature of COGs is\n",
    "that data is organized in “blocks” that can be accessed remotely via independent HTTP requests, abling partial file\n",
    "readings. This is useful if you want to access only a portion of your raster file, but it also lows for efficient\n",
    "parallel reading. You can check the blocksize employed in a COG file with the following code ippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b789c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "with rasterio.open(visual_href) as r:\n",
    "    if r.is_tiled:\n",
    "        print(f\"Chunk size: {r.block_shapes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227c2285",
   "metadata": {},
   "source": [
    "In order to optimally access COGs it is best to align the blocksize of the file with the chunks ployed when loading\n",
    "the file. Open the blue-band asset (the “blue” asset) of a Sentinel-2 scene as a chunked `DataArray` object using a suitable\n",
    "chunk size. Which elements do you think should be considered when choosing the chunk size?\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abc7ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "with rasterio.open(blue_band_href) as r:\n",
    "    if r.is_tiled:\n",
    "        print(f\"Chunk size: {r.block_shapes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abe2875",
   "metadata": {},
   "source": [
    "``` output\n",
    "Chunk size: [(1024, 1024)]\n",
    "```\n",
    "\n",
    "Ideal chunk size values for this raster are thus multiples of 1024. An element to consider is number of\n",
    "resulting chunks and their size. While the optimal chunk size strongly depends on the specific application, chunks\n",
    "should in general not be too big nor too small (i.e. too many). As a rule of thumb, chunk sizes of 100 MB typically\n",
    "work well with Dask (see, e.g., this [blog post](https://blog.dask.org/2021/11/02/choosing-dask-chunk-sizes)). Also,\n",
    "the shape might be relevant, depending on the application! Here, we might select a chunks shape of\n",
    "`(1, 6144, 6144)`::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90f68b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "band = rioxarray.open_rasterio(blue_band_href, chunks=(1, 6144, 6144))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6179bcc6",
   "metadata": {},
   "source": [
    "which leads to chunks 72 MB large: ((1 x 6144 x 6144) x 2 bytes / 2^20 = 72 MB). Also, we can t ioxarray`and Dask figure out appropriate chunk shapes by setting`chunks=“auto”\\`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "band = rioxarray.open_rasterio(blue_band_href, chunks=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca37b10",
   "metadata": {},
   "source": [
    "which leads to `(1, 8192, 8192)` chunks (128 MB).\n",
    "\n",
    "### Parallel computations\n",
    "\n",
    "Operations performed on a `DataArray` that has been opened as a chunked Dask array are executed using Dask. Dask\n",
    "coordinates how the operations should be executed on the individual chunks of data, and runs these tasks in parallel as\n",
    "much as possible.\n",
    "\n",
    "Let’s now repeat the raster calculations that we have carried out in the previous section, but running calculations in\n",
    "parallel over a multi-core CPU. We first open the relevant rasters as chunked arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16b508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_dask = rioxarray.open_rasterio(visual_href, overview_level=2, lock=False, chunks=(3, 500, 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b536c31",
   "metadata": {},
   "source": [
    "Setting `lock=False` tells `rioxarray` that the individual data chunks can be loaded simultaneously from the source by\n",
    "the Dask workers.\n",
    "\n",
    "As the next step, we trigger the download of the data using the `.persist()` method, see below. This makes sure that\n",
    "the downloaded chunks are stored in the form of a chunked Dask array (calling `.load()` would instead merge the chunks\n",
    "in a single Numpy array).\n",
    "\n",
    "We explicitly tell Dask to parallelize the required workload over 4 threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9732a6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_dask = visual_dask.persist(scheduler=\"threads\", num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105fa212",
   "metadata": {},
   "source": [
    "Let’s now continue to the actual calculation. Note how the same syntax as for its serial version is employed for\n",
    "applying the median filter. Don’t forget to add the Jupyter magic to record the timing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0082fec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "median_dask = visual_dask.rolling(x=7, y=7, center=True).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae85c46",
   "metadata": {},
   "source": [
    "``` output\n",
    "CPU times: user 20.6 ms, sys: 3.71 ms, total: 24.3 ms\n",
    "Wall time: 25.2 ms\n",
    "```\n",
    "\n",
    "Did we just observe a 700x speed-up when comparing to the serial calculation (19.6 s vs 25.2 ms)? Actually, no\n",
    "calculation has run yet. This is because operations performed on Dask arrays are executed “lazily”, i.e. they are not\n",
    "immediately run.\n",
    "\n",
    "### Dask graph\n",
    "\n",
    "The sequence of operations to carry out is stored in a task graph, which can be visualized with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8ad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "dask.visualize(median_dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b8c914",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"attachment:fig/E11/dask-graph.png\" alt=\"dask graph\" />\n",
    "<figcaption>Dask graph</figcaption>\n",
    "</figure>\n",
    "\n",
    "The task graph gives Dask the complete “overview” of the calculation, thus enabling a better nagement of tasks and\n",
    "resources when dispatching calculations to be run in parallel.\n",
    "\n",
    "Most methods of `DataArray`’s run operations lazily when Dask arrays are employed. In order to trigger calculations, we\n",
    "can use either `.persist()` or `.compute()`. The former keeps data in the form of chunked Dask arrays, and it should\n",
    "thus be used to run intermediate steps that will be followed by additional calculations. The latter merges instead the\n",
    "chunks in a single Numpy array, and it should be used at the very end of a sequence of calculations. Both methods\n",
    "accept the same parameters (here, we again explicitly tell Dask to run tasks on 4 threads). Let’s again time the cell\n",
    "execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2e4e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "median_dask = median_dask.persist(scheduler=\"threads\", num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e0e00c",
   "metadata": {},
   "source": [
    "``` output\n",
    "CPU times: user 19.1 s, sys: 3.2 s, total: 22.3 s\n",
    "Wall time: 6.61 s\n",
    "```\n",
    "\n",
    "The timing that we have recorded makes much more sense now. When running the task on a 4-core CPU laptop, we observe a\n",
    "x3 speed-up when comparing to the analogous serial calculation (19.6 s vs 6.61 s).\n",
    "\n",
    "Once again, we stress that one does not always obtain similar performance gains by exploiting the Dask-based\n",
    "parallelization. Even if the algorithm employed is well suited for parallelization, Dask introduces some overhead time\n",
    "to manage the tasks in the Dask graph. This overhead, which is typically of the order of few milliseconds per task, can\n",
    "be larger than the parallelization gain. This is the typical situation with calculations with many small chunks.\n",
    "\n",
    "Finally, let’s have a look at how Dask can be used to save raster files. When calling `.to_raster()`, we provide the\n",
    "following additional arguments:\n",
    "\\* `tiled=True`: write raster as a chunked GeoTIFF.\n",
    "\\* `lock=threading.Lock()`: the threads which are splitting the workload must “synchronise” when writing to the same file\n",
    "(they might otherwise overwrite each other’s output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170459ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Lock\n",
    "median_dask.rio.to_raster(\"visual_median-filter_dask.tif\", tiled=True, lock=Lock())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fba5de",
   "metadata": {},
   "source": [
    "Note that `.to_raster()` is among the methods that trigger immediate calculations (one can change this behaviour by\n",
    "specifying `compute=False`)\n",
    "\n",
    "-   The `%%time` Jupyter magic command can be used to profile calculations.\n",
    "-   Data ‘chunks’ are the unit of parallelization in raster calculations.\n",
    "-   (`rio`)`xarray` can open raster files as chunked arrays.\n",
    "-   The chunk shape and size can significantly affect the calculation performance.\n",
    "-   Cloud-optimized GeoTIFFs have an internal structure that enables performant parallel read."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
